{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Introduction to single-cell multi-view profiler (scMVP)\n",
    "In this introductory tutorial, we present the different tasks of a scMVP workflow\n",
    "1. Loading the multi-omics data\n",
    "2. Training the multi-view model\n",
    "3. Retrieving the common latent space and imputed multi-omics values\n",
    "4. Perform cell clustering and differential expression \n",
    "5. Visualize the common latent space and clustering with umap\n",
    "6. The differential gene cluster identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-09-11 15:49:33,564] INFO - scMVP._settings | Added StreamHandler with custom formatter to 'scMVP' logger.\n",
      "/home/fusl/miniconda2/envs/scMVP/lib/python3.7/site-packages/scikit_learn-0.22.2-py3.7-linux-x86_64.egg/sklearn/utils/linear_assignment_.py:22: FutureWarning: The linear_assignment_ module is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scMVP.dataset import scienceDataset\n",
    "from scMVP.models import VAE\n",
    "from scMVP.inference import UnsupervisedTrainer\n",
    "from scMVP.inference import MultiPosterior, MultiTrainer\n",
    "import torch\n",
    "from scMVP.models.multi_vae import Multi_VAE\n",
    "\n",
    "## Visualizing the latent space with scanpy\n",
    "import scanpy as sc\n",
    "import anndata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Loading data\n",
    "\n",
    " Loading the sci-CAR cell line dataset described in Junyue Cao et al. (2018).\n",
    "\n",
    "* Junyue Cao, et al. \"Joint profiling of chromatin accessibility and gene expression in thousands of single cells.\" Science 361.6409 (2018): 1380-1385. \n",
    "\n",
    "Data url: https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE117089&format=file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-09-11 15:49:34,761] INFO - scMVP.dataset.scienceDataset | Preprocessing dataset\n",
      "/home/fusl/miniconda2/envs/scMVP/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3417: DtypeWarning: Columns (2) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "[2020-09-11 15:49:50,158] INFO - scMVP.dataset.scienceDataset | Finished preprocessing dataset\n",
      "[2020-09-11 15:49:53,259] INFO - scMVP.dataset.dataset | Computing the library size for the new data\n",
      "[2020-09-11 15:49:54,060] INFO - scMVP.dataset.dataset | Downsampled from 4825 to 4825 cells\n"
     ]
    }
   ],
   "source": [
    "def allow_mmvae_for_test():\n",
    "    print(\"Testing the basic tutorial scMVP\")\n",
    "\n",
    "test_mode = False\n",
    "save_path = \"data/\"\n",
    "n_epochs_all = None\n",
    "show_plot = True\n",
    "\n",
    "if not test_mode:\n",
    "    save_path = \"dataset/\"\n",
    "dataset = scienceDataset(dataset_name=\"CellLineMixture\", save_path=save_path, measurement_names_column=1, is_binary = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data filtering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-09-11 15:49:54,794] INFO - scMVP.dataset.dataset | Downsampling from 113153 to 20403 genes\n",
      "[2020-09-11 15:49:55,259] INFO - scMVP.dataset.dataset | Computing the library size for the new data\n",
      "[2020-09-11 15:49:55,604] INFO - scMVP.dataset.dataset | Filtering non-expressing cells.\n",
      "[2020-09-11 15:49:56,574] INFO - scMVP.dataset.dataset | Computing the library size for the new data\n",
      "[2020-09-11 15:49:56,721] INFO - scMVP.dataset.dataset | Downsampled from 4825 to 4825 cells\n",
      "[2020-09-11 15:49:58,300] INFO - scMVP.dataset.dataset | Downsampling from 20403 to 10000 genes\n",
      "[2020-09-11 15:49:58,754] INFO - scMVP.dataset.dataset | Computing the library size for the new data\n",
      "[2020-09-11 15:49:58,923] INFO - scMVP.dataset.dataset | Filtering non-expressing cells.\n",
      "[2020-09-11 15:49:59,721] INFO - scMVP.dataset.dataset | Computing the library size for the new data\n",
      "[2020-09-11 15:49:59,794] INFO - scMVP.dataset.dataset | Downsampled from 4825 to 4825 cells\n",
      "[2020-09-11 15:50:00,864] INFO - scMVP.dataset.dataset | Computing the library size for the new data\n",
      "[2020-09-11 15:50:00,938] INFO - scMVP.dataset.dataset | Downsampled from 4825 to 4825 cells\n"
     ]
    }
   ],
   "source": [
    "def filter_dataset(dataset):\n",
    "    high_count_genes = (dataset.X > 0).sum(axis=0).ravel() > 0.01 * dataset.X.shape[0]\n",
    "    dataset.update_genes(high_count_genes)\n",
    "    dataset.subsample_genes(new_n_genes=10000)\n",
    "    \n",
    "    high_gene_count_cells = (dataset.X > 0).sum(axis=1).ravel() > 50\n",
    "    #high_atac_cells = dataset.atac_expression.sum(axis=1) >= np.percentile(dataset.atac_expression.sum(axis=1), 10)\n",
    "    high_atac_cells = dataset.atac_expression.sum(axis=1) >= np.percentile(dataset.atac_expression.sum(axis=1), 1)\n",
    "    inds_to_keep = np.logical_and(high_gene_count_cells, high_atac_cells)\n",
    "    dataset.update_cells(inds_to_keep)\n",
    "    return dataset, inds_to_keep\n",
    "\n",
    "if test_mode is False:\n",
    "    dataset, inds_to_keep = filter_dataset(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __n_epochs__: Maximum number of epochs to train the model. If the likelihood change is small than a set threshold training will stop automatically. \n",
    "* __lr__: learning rate. Set to 0.001 here. \n",
    "* __use_batches__: If the value of true than batch information is used in the training. Here it is set to false because the cortex data only contains one batch. \n",
    "* __use_cuda__: Set to true to use CUDA (GPU required) \n",
    "* __n_centroids__: Set the number of cell types\n",
    "* __n_alfa__: Set the weight of KL loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_epochs = 50 if n_epochs_all is None else n_epochs_all\n",
    "lr = 1e-3\n",
    "use_batches = False\n",
    "use_cuda = True\n",
    "n_centroids = 5\n",
    "n_alfa = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Pre-training\n",
    "runing pre-train vae to initialize the Gaussian Mixture Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (z_encoder): Encoder(\n",
       "    (encoder): FCLayers(\n",
       "      (fc_layers): Sequential(\n",
       "        (Layer 0): Sequential(\n",
       "          (0): Linear(in_features=10000, out_features=128, bias=True)\n",
       "          (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (mean_encoder): Linear(in_features=128, out_features=10, bias=True)\n",
       "    (var_encoder): Linear(in_features=128, out_features=10, bias=True)\n",
       "  )\n",
       "  (l_encoder): Encoder(\n",
       "    (encoder): FCLayers(\n",
       "      (fc_layers): Sequential(\n",
       "        (Layer 0): Sequential(\n",
       "          (0): Linear(in_features=10000, out_features=128, bias=True)\n",
       "          (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (mean_encoder): Linear(in_features=128, out_features=1, bias=True)\n",
       "    (var_encoder): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       "  (decoder): DecoderSCVI(\n",
       "    (px_decoder): FCLayers(\n",
       "      (fc_layers): Sequential(\n",
       "        (Layer 0): Sequential(\n",
       "          (0): Linear(in_features=266, out_features=128, bias=True)\n",
       "          (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "          (3): None\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (px_scale_decoder): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=10000, bias=True)\n",
       "      (1): Softmax(dim=-1)\n",
       "    )\n",
       "    (px_r_decoder): Linear(in_features=128, out_features=10000, bias=True)\n",
       "    (px_dropout_decoder): Linear(in_features=128, out_features=10000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fusl/miniconda2/envs/scMVP/lib/python3.7/site-packages/anndata-0.7.4-py3.7.egg/anndata/_core/anndata.py:1192: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  if is_string_dtype(df[key]) and not is_categorical(df[key])\n",
      "... storing 'cell_type' as categorical\n"
     ]
    }
   ],
   "source": [
    "pre_vae = VAE(dataset.nb_genes, n_batch=256)\n",
    "pre_trainer = UnsupervisedTrainer(\n",
    "    pre_vae,\n",
    "    dataset,\n",
    "    train_size=0.75,\n",
    "    use_cuda=use_cuda,\n",
    "    frequency=5\n",
    ")\n",
    "is_test_pragram = False\n",
    "if is_test_pragram:\n",
    "    pre_trainer.train(n_epochs=n_epochs, lr=lr)\n",
    "    torch.save(pre_trainer.model.state_dict(), '%s/pre_trainer6.pkl' % save_path)\n",
    "\n",
    "if os.path.isfile('%s/pre_trainer6.pkl' % save_path):\n",
    "    pre_trainer.model.load_state_dict(torch.load('%s/pre_trainer6.pkl' % save_path))\n",
    "    pre_trainer.model.eval()\n",
    "else:\n",
    "    #pre_trainer.model.init_gmm_params(dataset)\n",
    "    pre_trainer.train(n_epochs=n_epochs, lr=lr)\n",
    "    torch.save(pre_trainer.model.state_dict(), '%s/pre_trainer6.pkl' % save_path)\n",
    "\n",
    "# pretrainer_posterior:\n",
    "full = pre_trainer.create_posterior(pre_trainer.model, dataset, indices=np.arange(len(dataset)))\n",
    "latent, batch_indices, labels = full.sequential().get_latent()\n",
    "batch_indices = batch_indices.ravel()\n",
    "imputed_values = full.sequential().imputation()\n",
    "\n",
    "sample_latents = torch.tensor([])\n",
    "samples = torch.tensor([])\n",
    "sample_labels = torch.tensor([])\n",
    "for tensors_list in range(int(len(imputed_values)/256)+1):\n",
    "    if tensors_list == range(int(len(imputed_values)/256)):\n",
    "        x = torch.zeros((256,len(imputed_values[0])))\n",
    "        x[0:len(x)-256*tensors_list,:] = torch.tensor(imputed_values[tensors_list * 256:len(imputed_values), :])\n",
    "        y = torch.zeros((256))\n",
    "        y[0:len(x)-256*tensors_list,:]  = torch.tensor(dataset.labels[tensors_list * 256:len(imputed_values)].astype(int))\n",
    "        temp_samples = pre_trainer.model.get_latents(x,y)\n",
    "        for temp_sample in temp_samples:\n",
    "            sample_latents = torch.cat((sample_latents, temp_sample[0:len(x)-256*tensors_list,:].float()))\n",
    "    temp_samples = pre_trainer.model.get_latents(\n",
    "        x=torch.tensor(imputed_values[tensors_list * 256:(1 + tensors_list) * 256, :]),\n",
    "        y=torch.tensor(dataset.labels[tensors_list * 256:(1 + tensors_list) * 256].astype(int)))\n",
    "    for temp_sample in temp_samples:\n",
    "        sample_latents = torch.cat((sample_latents, temp_sample.float()))\n",
    "        \n",
    "# visulization\n",
    "prior_adata = anndata.AnnData(X=dataset.X)\n",
    "prior_adata.obsm[\"X_multi_vi\"] = sample_latents.detach().numpy()\n",
    "prior_adata.obs['cell_type'] = torch.tensor(dataset.labels[0:len(sample_latents)].astype(int))\n",
    "sc.pp.neighbors(prior_adata, use_rep=\"X_multi_vi\", n_neighbors=15)\n",
    "sc.tl.umap(prior_adata, min_dist=0.1)\n",
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "sc.pl.umap(prior_adata, color=[\"cell_type\"], ax=ax, show=show_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training scMVP\n",
    "We now create the scMVP model and the trainer object.\n",
    "\n",
    "If a pre-trained model already exist in the save_path then load the same model rather than re-training it. This is particularly useful for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Multi_VAE(\n",
       "  (RNA_encoder): Encoder(\n",
       "    (encoder): FCLayers(\n",
       "      (fc_layers): Sequential(\n",
       "        (Layer 0): Sequential(\n",
       "          (0): Linear(in_features=10000, out_features=128, bias=True)\n",
       "          (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (mean_encoder): Linear(in_features=128, out_features=10, bias=True)\n",
       "    (var_encoder): Linear(in_features=128, out_features=10, bias=True)\n",
       "  )\n",
       "  (ATAC_encoder): Encoder(\n",
       "    (encoder): FCLayers(\n",
       "      (fc_layers): Sequential(\n",
       "        (Layer 0): Sequential(\n",
       "          (0): Linear(in_features=57919, out_features=128, bias=True)\n",
       "          (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (mean_encoder): Linear(in_features=128, out_features=10, bias=True)\n",
       "    (var_encoder): Linear(in_features=128, out_features=10, bias=True)\n",
       "  )\n",
       "  (RNA_ATAC_encoder): Multi_Encoder(\n",
       "    (scRNA_encoder): FCLayers(\n",
       "      (fc_layers): Sequential(\n",
       "        (Layer 0): Sequential(\n",
       "          (0): Linear(in_features=10000, out_features=128, bias=True)\n",
       "          (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (scATAC_encoder): FCLayers(\n",
       "      (fc_layers): Sequential(\n",
       "        (Layer 0): Sequential(\n",
       "          (0): Linear(in_features=57919, out_features=128, bias=True)\n",
       "          (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (concat1): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (concat2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (mean_encoder): Linear(in_features=128, out_features=10, bias=True)\n",
       "    (var_encoder): Linear(in_features=128, out_features=10, bias=True)\n",
       "  )\n",
       "  (RNA_ATAC_decoder): Multi_Decoder(\n",
       "    (scRNA_decoder): FCLayers(\n",
       "      (fc_layers): Sequential(\n",
       "        (Layer 0): Sequential(\n",
       "          (0): Linear(in_features=266, out_features=128, bias=True)\n",
       "          (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "          (3): None\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (rna_scale_decoder): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=10000, bias=True)\n",
       "      (1): Softmax(dim=-1)\n",
       "    )\n",
       "    (rna_r_decoder): Linear(in_features=128, out_features=10000, bias=True)\n",
       "    (rna_dropout_decoder): Linear(in_features=128, out_features=10000, bias=True)\n",
       "    (scATAC_decoder): FCLayers(\n",
       "      (fc_layers): Sequential(\n",
       "        (Layer 0): Sequential(\n",
       "          (0): Linear(in_features=266, out_features=128, bias=True)\n",
       "          (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "          (3): None\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (atac_scale_decoder): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=57919, bias=True)\n",
       "      (1): Softmax(dim=-1)\n",
       "    )\n",
       "    (atac_r_decoder): Linear(in_features=128, out_features=57919, bias=True)\n",
       "    (atac_dropout_decoder): Linear(in_features=128, out_features=57919, bias=True)\n",
       "    (libaray_decoder): FCLayers(\n",
       "      (fc_layers): Sequential(\n",
       "        (Layer 0): Sequential(\n",
       "          (0): Linear(in_features=266, out_features=128, bias=True)\n",
       "          (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "          (3): None\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (libaray_rna_scale_decoder): Linear(in_features=128, out_features=1, bias=True)\n",
       "    (libaray_atac_scale_decoder): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_vae = Multi_VAE(dataset.nb_genes, len(dataset.atac_names), n_batch=256, n_centroids=n_centroids, n_alfa = n_alfa, mode=\"mm-vae\") # should provide ATAC num, alfa, mode and loss type\n",
    "trainer = MultiTrainer(\n",
    "    multi_vae,\n",
    "    dataset,\n",
    "    train_size=0.75,\n",
    "    use_cuda=use_cuda,\n",
    "    frequency=5,\n",
    ")\n",
    "\n",
    "clust_index_gmm = trainer.model.init_gmm_params(sample_latents.detach().numpy())\n",
    "\n",
    "is_test_pragram = False\n",
    "if is_test_pragram:\n",
    "    trainer.train(n_epochs=n_epochs, lr=lr)\n",
    "    torch.save(trainer.model.state_dict(), '%s/multi_vae_21.pkl' % save_path)\n",
    "if os.path.isfile('%s/multi_vae_21.pkl' % save_path):\n",
    "    trainer.model.load_state_dict(torch.load('%s/multi_vae_21.pkl' % save_path))\n",
    "    trainer.model.eval()\n",
    "else:\n",
    "    trainer.train(n_epochs=n_epochs, lr=lr)\n",
    "    torch.save(trainer.model.state_dict(), '%s/multi_vae_21.pkl' % save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Plotting the likelihood change across the n epochs of training: blue for training error and orange for testing error.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7feec8463e50>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7feec8463a90>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(1150.0, 1600.0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## If you train your own model, you can plot the elbo value during training.\n",
    "## If your load pre-trained model, the plot would be empty.\n",
    "\n",
    "elbo_train_set = trainer.history[\"elbo_train_set\"]\n",
    "elbo_test_set = trainer.history[\"elbo_test_set\"]\n",
    "x = np.linspace(0, 500, (len(elbo_train_set)))\n",
    "plt.plot(x, elbo_train_set)\n",
    "plt.plot(x, elbo_test_set)\n",
    "plt.ylim(1150, 1600)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "query the imputed values via the `imputation` method of the posterior object and get common latent embedding. **Note for advanced users:** imputation is an ambiguous term and there are two ways to perform imputation in scVI. The first way is to query the **mean of the negative binomial** distribution modeling the counts. This is referred to as `sample_rate` in the codebase and can be reached via the `imputation` method. The second is to query the **normalized mean of the same negative binomial** (please refer to the scVI manuscript). This is referred to as `sample_scale` in the codebase and can be reached via the `get_sample_scale` method. In differential expression for example, we of course rely on the normalized latent variable which is corrected for variations in sequencing depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "full = trainer.create_posterior(trainer.model, dataset, indices=np.arange(len(dataset)),type_class=MultiPosterior)\n",
    "imputed_values = full.sequential().imputation()\n",
    "sample_latents = torch.tensor([])\n",
    "sample_labels = torch.tensor([])\n",
    "rna_imputation = imputed_values[0]\n",
    "atac_imputation = imputed_values[3]\n",
    "temp_label = []\n",
    "sample_latents = torch.tensor([])\n",
    "samples = torch.tensor([])\n",
    "sample_labels = torch.tensor([])\n",
    "if len(imputed_values) >= 3:\n",
    "    temp_label = imputed_values[2]\n",
    "for tensors_list in range(int(len(imputed_values[0])/256)+1):\n",
    "    if temp_label.any():\n",
    "        temp_samples = trainer.model.get_latents(x_rna=torch.tensor(rna_imputation[tensors_list*256:(1+tensors_list)*256,:]),\n",
    "                                                x_atac=torch.tensor(atac_imputation[tensors_list*256:(1+tensors_list)*256,:]),\n",
    "                                                y=torch.tensor(temp_label[tensors_list*256:(1+tensors_list)*256])) \n",
    "    else:\n",
    "        temp_samples = trainer.model.get_latents(x_rna=torch.tensor(rna_imputation[tensors_list*256:(1+tensors_list)*256,:]),\n",
    "                                                x_atac=torch.tensor(atac_imputation[tensors_list*256:(1+tensors_list)*256,:]),\n",
    "                                                y=torch.tensor(np.zeros(256))) \n",
    "    for temp_sample in temp_samples:\n",
    "        #sample_latents = torch.cat((sample_latents, temp_sample[2].float()))\n",
    "        sample_latents = torch.cat((sample_latents, temp_sample[0][0].float()))\n",
    "        sample_labels = torch.cat((sample_labels, torch.tensor(temp_label[tensors_list*256:(1+tensors_list)*256]).float()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Cell clustering\n",
    "Perform cell clustering and merging the rare clusters which less than 10 cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "clust_index_gmm = trainer.model.init_gmm_params(sample_latents.detach().numpy())\n",
    "gmm_clus_index = clust_index_gmm.reshape(-1,1)\n",
    "for i in range(len(np.unique(gmm_clus_index))):\n",
    "    if len(gmm_clus_index[gmm_clus_index == i]) <= 10:\n",
    "        for j in range(len(np.unique(gmm_clus_index))):\n",
    "            if len(gmm_clus_index[gmm_clus_index == j]) > 100:\n",
    "                gmm_clus_index[gmm_clus_index == i] = j\n",
    "                break\n",
    "unique_gmm_clus_index = np.unique(gmm_clus_index)\n",
    "for i in range(len(unique_gmm_clus_index)):\n",
    "    gmm_clus_index[gmm_clus_index == unique_gmm_clus_index[i]] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing common latent embedding and cell clustering by scMVP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fusl/miniconda2/envs/scMVP/lib/python3.7/site-packages/anndata-0.7.4-py3.7.egg/anndata/_core/anndata.py:1192: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  if is_string_dtype(df[key]) and not is_categorical(df[key])\n",
      "... storing 'cell_type' as categorical\n",
      "/home/fusl/miniconda2/envs/scMVP/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n",
      "/home/fusl/miniconda2/envs/scMVP/lib/python3.7/site-packages/anndata-0.7.4-py3.7.egg/anndata/_core/anndata.py:1192: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  if is_string_dtype(df[key]) and not is_categorical(df[key])\n",
      "... storing 'cell_type' as categorical\n"
     ]
    }
   ],
   "source": [
    "posterior_adata = anndata.AnnData(X=rna_imputation)\n",
    "posterior_adata.obsm[\"X_multi_vi\"] = sample_latents.detach().numpy()\n",
    "posterior_adata.obs['cell_type'] = torch.tensor(clust_index_gmm.reshape(-1,1))\n",
    "sc.pp.neighbors(posterior_adata, use_rep=\"X_multi_vi\", n_neighbors=15)\n",
    "sc.tl.umap(posterior_adata, min_dist=0.1)\n",
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "sc.pl.umap(posterior_adata, color=[\"cell_type\"], ax=ax, show=show_plot)\n",
    "# imputation labels\n",
    "posterior_adata.obs['cell_type'] = torch.tensor(sample_labels.reshape(-1,1))\n",
    "sc.pp.neighbors(posterior_adata, use_rep=\"X_multi_vi\", n_neighbors=15)\n",
    "sc.tl.umap(posterior_adata, min_dist=0.1)\n",
    "#matplotlib.use('TkAgg')\n",
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "sc.pl.umap(posterior_adata, color=[\"cell_type\"], ax=ax, show=show_plot)\n",
    "sc.tl.louvain(posterior_adata)\n",
    "sc.pl.umap(posterior_adata, color=['louvain'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## differential gene and peak analysis\n",
    "Identification differential genes and peaks in each cell cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Default of the method has been changed to 't-test' from 't-test_overestim_var'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "... storing 'louvain' as categorical\n"
     ]
    }
   ],
   "source": [
    "posterior_adata.obs['louvain'] = torch.tensor(gmm_clus_index.reshape(-1,1))\n",
    "sc.tl.rank_genes_groups(posterior_adata, 'louvain')\n",
    "sc.pl.rank_genes_groups(posterior_adata, n_genes=10, sharey=False)\n",
    "diff_top_gene_set = posterior_adata.uns['rank_genes_groups']\n",
    "diff_top_gene_set = (diff_top_gene_set['names'])\n",
    "diff_top_gene_pvalue_set = posterior_adata.uns['rank_genes_groups']\n",
    "diff_top_gene_pvalue_set = (diff_top_gene_pvalue_set['pvals_adj'])\n",
    "diff_top_gene_foldchange_set = posterior_adata.uns['rank_genes_groups']\n",
    "diff_top_gene_foldchange_set = (diff_top_gene_foldchange_set['logfoldchanges'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Default of the method has been changed to 't-test' from 't-test_overestim_var'\n"
     ]
    }
   ],
   "source": [
    "atac_posterior_adata = anndata.AnnData(X=atac_imputation)\n",
    "atac_posterior_adata.obs['louvain'] = posterior_adata.obs['louvain']\n",
    "sc.tl.rank_genes_groups(atac_posterior_adata, 'louvain',n_genes=1000)\n",
    "sc.pl.rank_genes_groups(atac_posterior_adata, n_genes=10, sharey=False)\n",
    "atac_diff_top_gene_set = atac_posterior_adata.uns['rank_genes_groups']\n",
    "atac_diff_top_gene_set = (atac_diff_top_gene_set['names'])\n",
    "atac_diff_top_gene_pvalue_set = atac_posterior_adata.uns['rank_genes_groups']\n",
    "atac_diff_top_gene_pvalue_set = (atac_diff_top_gene_pvalue_set['pvals_adj'])\n",
    "atac_diff_top_gene_foldchange_set = atac_posterior_adata.uns['rank_genes_groups']\n",
    "atac_diff_top_gene_foldchange_set = (atac_diff_top_gene_foldchange_set['logfoldchanges'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
